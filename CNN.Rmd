---
title: "CNN"
author: "Alex Liu"
date: "2023-04-17"
output: html_document
---

```{r warning=F, message=F}
library(EBImage)
library(tidyverse)
library(pracma)
library(randomForest)
library(ggimage)
```

```{r}
labels = list.files("../bio_data/Biotechnology/data_processed/cell_images/")
cell_boundaries_raw = read.csv("../bio_data/Biotechnology/data_processed/cell_boundaries.csv.gz")

cluster_ids = list()

for (i in 1:length(labels)) {
  
  cluster_file = list.files(paste0("../bio_data/Biotechnology/data_processed/cell_images/", labels[i]))
  
  cluster_ids[i] <- list(gsub(".*cell_|.png", "", cluster_file))
}

cluster_imgs = list()

for (i in 1:length(labels)) {
  
  cluster_file = list.files(paste0("../bio_data/Biotechnology/data_processed/cell_images/", labels[i]), full.names=T)
  
  cluster_imgs[i] <- list(sapply(cluster_file, readImage, simplify = F))

}
```

```{r}
get_inside = function(cellID, img, cell_boundaries) {
  
  cell_boundary = cell_boundaries |>
    filter(cell_id %in% cellID)
  
  # rescale the boundary according to the pixels
  pixels = dim(img)
  cell_boundary$vertex_x_scaled <- 1+((cell_boundary$vertex_x - min(cell_boundary$vertex_x))/0.2125)
  cell_boundary$vertex_y_scaled <- 1+((cell_boundary$vertex_y - min(cell_boundary$vertex_y))/0.2125)
  
  # identify which pixels are inside or outside of the cell segment using inpolygon
  pixel_locations = expand.grid(seq_len(nrow(img)), seq_len(ncol(img)))
  
  pixels_inside = inpolygon(x = pixel_locations[,1],
                            y = pixel_locations[,2],
                            xp = cell_boundary$vertex_x_scaled,
                            yp = cell_boundary$vertex_y_scaled,
                            boundary = TRUE)
  
  img_inside = img
  img_inside@.Data <- matrix(pixels_inside, nrow = nrow(img), ncol = ncol(img))
  
  return(img_inside)
}

mask_resize = function(img, img_inside, w = 50, h = 50) {
  
  img_mask = img*img_inside
  
  # then, transform the masked image to the same number of pixels, 50x50
  img_mask_resized = resize(img_mask, w, h)
  
  return(img_mask_resized)
}
```

```{r}
cluster_imgs_masked_resized = list()

for (i in 1:length(labels)) {
  cell_boundaries = cell_boundaries_raw |>
    filter(cell_id %in% cluster_ids[[i]])
    
  cluster_imgs_inside <- mapply(get_inside, cluster_ids[[i]], cluster_imgs[[i]], MoreArgs = list(cell_boundaries = cell_boundaries), SIMPLIFY = FALSE)
  
  cluster_imgs_masked_resized[i] = list(mapply(mask_resize, cluster_imgs[[i]], cluster_imgs_inside, MoreArgs = list(w = 64, h = 64), SIMPLIFY = FALSE))

}
```

```{r}
set.seed(63)
library(keras)
use_condaenv("r-reticulate", required=T)
tensorflow::set_random_seed(63)

imgs_masked_resize_64 = do.call(c, cluster_imgs_masked_resized)

num_images = length(imgs_masked_resize_64)
img_names = names(imgs_masked_resize_64)

x = array(dim = c(num_images, 64, 64, 1))

# Centering Intensity values
for (i in 1:num_images) {
  x[i,,,1] <- imgs_masked_resize_64[[i]]@.Data - mean(imgs_masked_resize_64[[i]]@.Data)
}

input_shape = dim(x)[2:4]
```

```{r building_model}
model_function <- function(learning_rate = 0.001) {
  
  k_clear_session()
  
  model <- keras_model_sequential() %>%
    layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu', input_shape = input_shape) %>% 
    layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
    layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% 
    layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
    layer_dropout(rate = 0.25) %>% 
    layer_flatten() %>% 
    layer_dense(units = 128, activation = 'relu') %>% 
    layer_dropout(rate = 0.5) %>% 
    layer_dense(units = 64) %>% 
    layer_dropout(rate = 0.5) %>% 
    layer_dense(units = 28, activation = 'softmax')
  
  model %>% compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_adam(learning_rate = learning_rate),
    metrics = "accuracy"
  )
  
  return(model)
  
}

# Let this model compile and inspect its architecture

model_catent <- model_function()
```

```{r training_data}
times = c()

for (i in 1:length(labels)) {
  times[i] = length(cluster_ids[[i]])
}

y = factor(rep(labels, times = times))

batch_size <- 32
epochs <- 100

yy = model.matrix(~ y - 1)

set.seed(63)
shuf_ind <- sample(1:dim(x)[1], dim(x)[1]*0.8)
shuf_x_train <- x[shuf_ind, , ,]
shuf_x_train <- array(shuf_x_train, dim = c(dim(shuf_x_train)[1], dim(shuf_x_train)[2], dim(shuf_x_train)[3], 1))

shuf_x_test <- x[-shuf_ind, , ,]
shuf_x_test <- array(shuf_x_test, dim = c(dim(shuf_x_test)[1], dim(shuf_x_test)[2], dim(shuf_x_test)[3], 1))

shuf_yy_train <- yy[shuf_ind, ]
shuf_yy_test <- yy[-shuf_ind, ]
```

```{r class_weights}
# Addressing class imbalance through using class weights
clcount <- table(y[shuf_ind])

class_weight <- list(
  "0" = sum(clcount) / (28*clcount["cluster_1"]),
  "1" = sum(clcount) / (28*clcount["cluster_10"]),
  "2" = sum(clcount) / (28*clcount["cluster_11"]),
  "3" = sum(clcount) / (28*clcount["cluster_12"]),
  "4" = sum(clcount) / (28*clcount["cluster_13"]),
  "5" = sum(clcount) / (28*clcount["cluster_14"]),
  "6" = sum(clcount) / (28*clcount["cluster_15"]),
  "7" = sum(clcount) / (28*clcount["cluster_16"]),
  "8" = sum(clcount) / (28*clcount["cluster_17"]),
  "9" = sum(clcount) / (28*clcount["cluster_18"]),
  "10" = sum(clcount) / (28*clcount["cluster_19"]),
  "11" = sum(clcount) / (28*clcount["cluster_2"]),
  "12" = sum(clcount) / (28*clcount["cluster_20"]),
  "13" = sum(clcount) / (28*clcount["cluster_21"]),
  "14" = sum(clcount) / (28*clcount["cluster_22"]),
  "15" = sum(clcount) / (28*clcount["cluster_23"]),
  "16" = sum(clcount) / (28*clcount["cluster_24"]),
  "17" = sum(clcount) / (28*clcount["cluster_25"]),
  "18" = sum(clcount) / (28*clcount["cluster_26"]),
  "19" = sum(clcount) / (28*clcount["cluster_27"]),
  "20" = sum(clcount) / (28*clcount["cluster_28"]),
  "21" = sum(clcount) / (28*clcount["cluster_3"]),
  "22" = sum(clcount) / (28*clcount["cluster_4"]),
  "23" = sum(clcount) / (28*clcount["cluster_5"]),
  "24" = sum(clcount) / (28*clcount["cluster_6"]),
  "25" = sum(clcount) / (28*clcount["cluster_7"]),
  "26" = sum(clcount) / (28*clcount["cluster_8"]),
  "27" = sum(clcount) / (28*clcount["cluster_9"])
)
```

```{r}
hist <- model %>% fit(
  x = shuf_x_train,
  y = shuf_yy_train,
  batch_size = batch_size,
  steps_per_epoch = (dim(shuf_x_train)[1]*0.8) %/% batch_size,
  #class_weight = class_weight,
  epochs = epochs, 
  validation_split = 0.2,
  verbose = 2
)

plot(hist)

save_model_tf(model, "cnn")
```

```{r noise}
add_gaussian_noise <- function(images, mean = 0, sd = 0.1) {
  noisy_images <- array(0, dim = dim(images))
  for (i in 1:nrow(images)) {
    noisy_images[i,,,] <- images[i,,,] + rnorm(1, mean, sd)
  }
  return(noisy_images)
}

add_gaussian_noise_2 <- function(images, mean = 0, sd = 0.1) {
  noisy_images <- array(0, dim = dim(images))
  for (i in 1:nrow(images)) {
    noisy_images[i, ] <- images[i,] + rnorm(1, mean, sd)
  }
  return(noisy_images)
}
```

```{r}
noise_x = array(dim = c(num_images, 64, 64, 1))

for (i in 1:num_images) {
  noise_x[i,,,1] <- add_gaussian_noise_2(imgs_masked_resize_64[[i]]@.Data - mean(imgs_masked_resize_64[[i]]@.Data), mean = 0.2)
}

noise_x_train = noise_x[shuf_ind, , ,]
noise_x_train <- array(noise_x_train, dim = c(dim(noise_x_train)[1], dim(noise_x_train)[2], dim(noise_x_train)[3], 1))

x_train = abind(shuf_x_train, noise_x_train, along = 1)
y_train = abind(shuf_yy_train, shuf_yy_train, along = 1)

hist <- model_catent %>% fit(
  x = x_train,
  y = y_train,
  batch_size = batch_size,
  steps_per_epoch = (dim(x_train)[1]*0.8) %/% batch_size,
  #class_weight = class_weight,
  epochs = epochs, 
  validation_split = 0.2,
  verbose = 2
)

plot(hist)

save_model_tf(model_catent, "cnn_catent")
```








