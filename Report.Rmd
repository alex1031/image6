---
title: "Final Report - Image 6"
author: "names"
output:
  pdf_document:
    df_print: paged
    pandoc_args: []
geometry: margin=2cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(fig.pos = '!ht')
library(tidyverse)
library(ggpubr)
library(png)
library(EBImage)
val_loss <- read.csv("Image_App/val_loss.csv")
```

# Executive Summary

Short description of the problem.
The main findings.
Key figure if appropriate.
The practical relevance of the analysis.



# Aim and Background

A clear description of the problem, articulating the aim of this project.
Provides appropriate multidisciplinary context and motivational background explained well in an appropriate language. 
Including background of the data



# Method - Data Collection & Developed Models

The dataset was generated by 10x Genomics Xenium instrument on a fresh frozen mouse brain coronal section. The data bundle contained various components, including a cell morphology image where the intensity corresponded to the presence of the nucleus in each cell, cell boundaries indicating the spatial locations of the detected cells, and RNA abundances for each cell. The cells were then grouped into 28 distinct clusters, and the cluster labels were provided (Reference to data bundle). An Rmarkdown file was employed to generate the images. This process involved utilising the cell boundaries of the detected cells along with their corresponding labels to create per-cell images for a randomly selected subset of 1000 cells.

Prior to introsucing augmentation, several preprocessing steps were employed to maximize the effectiveness of our approach. These steps were undertaken to optimize the dataset before the introduction of data augmentation. The preprocessing steps included the exclusion of noise beyond the cell boundary vertices associated with the cells. This process ensured that the resulting images exclusively contained the cell itself, without any extra elements or noise. Additionally, the images were subjected to a mask and resize, ensuring that each cell shared a uniform size and eliminated any inconsistencies in the image generation process. The pixel intensities were also centered in order to maintain consistency across all images. By considering the substantial number of cell clusters present in the dataset, the image data was augmented by doubling the samples from 1000 to 2000. This augmentation technique was implemented to expand the sample size while maintaining a minimal increase in training time.

## Baseline Model

Initially, a Random Forest classifier was employed, utilising the pixel values of the provided images, to predict the corresponding cell clusters. However, the limited complexity of the model hindered its capability to capture the composite shapes of the cells, resulting in a error rate of 95%. Also, the high number of clusters within the dataset, totaling 28, further constrained its predictive performance. The reliance of handcrafted features or simple statistical measures from the Random Forest classifier was not effective in capturing the complex and abstract features present in images, leading to sub optimal performance in image classification tasks.

In order to address these issues, convolutional neural networks were employed. These neural networks utilise parameterised, sparsely connected kernels which preserve the spatial characterics of images, which allowed complex features to be captured (Shorten & Khoshgoftaar, 2019). Furthermore, CNNs can be built on pre-existing networks, which can be retrained for new recognition tasks. These advantages open up new opportunities to use CNNs for real-world applications without increasing computational complexities or costs (CNN-Example - Databricks, 2023).

The CNN model architecture implemented in this study comprised of two convolutional layers. The first convolutional layer consisted of 32 filters, followed by a pooling layer. Subsequently, the second convolutional layer was comprised of 64 filters, followed by another pooling layer. The output from the second convolutional layer was then flattened into a one-dimensional form, preparing it for the subsequent classification process. For classification, fully connected layers and dropout layers were employed to mitigate overfitting and enhance generalization capabilities. There were in total 1,632,962 trainable parameters available. A binary cross-entropy loss function was employed, along with the Adam optimiser. Prior to feeding the image into the CNN model, a transformation step is required to convert the images into a four-dimensional array. This array consists of two size 64 arrays, which correspond to the dimensions of the image. Typically, for RGB images, three size 64 arrays are utilized. However, for black and white images, two size 64 arrays are sufficient to represent the image data. (refer to literature). Given the constraints of limited computational resources, a shallow architecture was deliberately chosen for this study. This decision aimed to accommodate the resource limitations associated with deeper CNN networks and expedite the training process, ultimately achieving interpretable results within the given constraints.

## Model Variations

These model variations offer alternative and more dependable viewpoints regarding the impact of data augmentations on CNN models. They also provide a platform for exploring optimal configurations and settings for future studies in this field. By examining different approaches and their outcomes, these variations contribute to a deeper understanding of the role and effectiveness of data augmentations in CNN models. In total, 6 models were used for each augmentation.

### Class Weights

In this particular model variation, class weights were introduced during the training process. This approach was adopted to address the substantial class imbalance observed in the training set. By assigning class weights, greater emphasis was placed on classes with a lower count, while reducing the importance of classes with a higher count. To ensure flexibility and applicability in future implementations, the class weights were determined based on a ratio between the count of a specific class label and the total class count. (some literature for class weights)

### Categorical Cross-Entropy Loss Function

For this specific model variation, a transition was made from using binary cross-entropy to categorical cross-entropy as the loss function. Although both loss functions share similarities, categorical cross-entropy is more commonly employed in multi-class classification scenarios involving more than two classes (reference to literature). It quantifies the dissimilarity between the predicted class probabilities and the true class labels. This differs from binary cross-entropy, which measures the disparity between the predicted probabilities and the true labels. It can be seen later in the results section that the validation loss produced by these two loss functions were required to be interpreted in two different scales.

### RMSprop Optimiser

Finally, this model variation utilises RMSprop optimiser instead of the Adam optimiser in the original model. These optimisers differ in many aspects, including their update rule, bias correction and convergence methods. (From this literature study), although Adam is generally considered to have faster convergence and better performance on a wide range of tasks, it may be more sensitive to hyper parameter settings and can sometimes overfit or converge to suboptimal solutions. RMSprop, being a simpler algorithm, may be more robust in certain cases and requires fewer hyper parameters to tune.


# Method - Evaluation Strategies 

```{r eval=T, eval_Strat, echo=F, out.width="70%", out.height="20%",fig.show='hold',fig.align='center', fig.cap="Overview of Evaluation Strategy."}
knitr::include_graphics("images/eval_strategy.png")
```


During the implementation process, the augmented training set was incorporated alongside the original training set. The rotating and resizing augmentations were performed using the EBImage package (reference to EBImage package). To introduce Gaussian noise, a randomly generated number from a normal distribution with a specified mean and a standard deviation of 0.2 was added to each image. The augmented training set was subsequently utilised in training each variation of the CNN model, with the validation loss recorded for each epoch and stored in a CSV file for further analysis and evaluation.

To comprehensively assess the impact of each augmentation, different noise levels were carefully selected. For Gaussian noise, three distinct subcategories were incorporated: low (0.2 mean), high (0.8 mean), and random (0.2 to 1 mean). Similarly, the same approach was applied to rotation and resizing augmentations. Three subcategories were defined: low (90 degrees, 16x16), high/medium (180 degrees, 32x32), and random (1 to 359 degrees, low or medium resolution). Resizing were limited to below 64 due to the lack of improvement of quality past that. By adopting this strategy, a more insightful analysis of the effect of each augmentation on the model's performance and robustness was facilitated. All model were trained for 100 epochs in order to maintain consistency.

Finally, the level that produced the lowest validation for each augmentation was combined to form one single training set, adding up to 6000 samples, which was then fed into the 6 different CNN model variations. This allowed for the exploration on how data augmentation can used to improve the robustness of the model to variations in input image quality. The evaluation process was illustrated through Figure 1.

# Results - Effect of Data Augmentation

```{r gausshist, eval=T, include=T, echo=F, fig.width = 15, fig.height = 15, align="center", fig.cap="Bar Charts of Validation Loss for Different Gaussian Noise Levels on CNN Models. Top row includes the charts for Gaussian noise, middle row include charts for Resolution, and bottom row includes charts for rotation.", fig.pos="H"}
level_order <- c("none", "low", "high", "random")

gaussian <- val_loss |> filter((noise_type == "gaussian" | noise_type == "none") & !grepl("Category", model))
gaussian_catent <- val_loss |> filter((noise_type == "gaussian" | noise_type == "none") & grepl("Category", model))

p1 <- ggplot(data = gaussian, aes(x = factor(noise_level, level = level_order), y = val_loss, fill = model)) +
       geom_bar(stat = "identity", position="dodge") +
       xlab("Gaussian Noise Level") + ylab("Validation Loss") +
       ggtitle("Comparison of Validation Loss on Binary and RMSprop Model")
p2 <- ggplot(data = gaussian_catent, aes(x = factor(noise_level, level = level_order), y = val_loss, fill = model)) + 
         geom_bar(stat = "identity", position="dodge") +
         xlab("Gaussian Noise Level") + ylab("Validation Loss") +
         ggtitle("Comparison of Validation Loss on Categorical Model")

level_order_res <- c("none", "low", "medium", "random")
    
resolution <- val_loss |> filter((noise_type == "resolution" | noise_type == "none") & !grepl("Category", model))
resolution_catent <- val_loss |> filter((noise_type == "resolution" | noise_type == "none") & grepl("Category", model))
    
p3 <- ggplot(data = resolution, aes(x = factor(noise_level, level = level_order_res), y = val_loss, fill = model)) +
       geom_bar(stat = "identity", position="dodge") +
       xlab("Resolution") + ylab("Validation Loss") +
       ggtitle("Comparison of Validation Loss on Binary and RMSprop Model")

p4 <- ggplot(data = resolution_catent, aes(x = factor(noise_level, level = level_order_res), y = val_loss, fill = model)) + 
               geom_bar(stat = "identity", position="dodge") +
               xlab("Resolution") + ylab("Validation Loss") +
               ggtitle("Comparison of Validation Loss on Categorical Model")

level_order <- c("none", "low", "high", "random")
    
resolution <- val_loss |> filter((noise_type == "rotation" | noise_type == "none") & !grepl("Category", model))

p5 <- ggplot(data = resolution, aes(x = factor(noise_level, level = level_order), y = val_loss, fill = model)) +
       geom_bar(stat = "identity", position="dodge") +
       xlab("Rotation") + ylab("Validation Loss") +
       ggtitle("Comparison of Validation Loss on Binary and RMSprop Model")

resolution_catent <- val_loss |> filter((noise_type == "rotation" | noise_type == "none") & grepl("Category", model))
    
p6 <- ggplot(data = resolution_catent, aes(x = factor(noise_level, level = level_order), y = val_loss, fill = model)) + 
       geom_bar(stat = "identity", position="dodge") +
       xlab("Rotation") + ylab("Validation Loss") +
       ggtitle("Comparison of Validation Loss on Categorical Model")

plot(ggarrange(p1, p2, p3, p4, p5, p6,
          ncol = 2, nrow=3))
```

## Gaussian Noise

Observing Figure 2 (Top Row), the introduction of Gaussian noise in the implementation resulted in an increase in validation loss across all models, as compared to the original model. Notably, the application of random mean Gaussian noise yielded the largest validation loss for both the binary and categorical models. Moreover, a high mean Gaussian noise of 0.8 was applied led to the highest validation loss for both the binary and categorical models with class weights. Finally, the inclusion of low mean Gaussian noise demonstrated the highest validation loss for the RMSprop models, both with and without class weights.

## Resolution

The incorporation of low (16x16) and medium (32x32) resolutions in the training set resulted in a reduction of validation loss across all models, as illustrated in Figure 2 (Middle Row). The categorical models with class weights experienced the most substantial benefits from this augmentation, with a decrease of 0.7503 when low resolution was added and a decrease of 1.0054 when medium resolution was utilized. Conversely, the introduction of random resolution into the training set led to an increase in validation loss, particularly noticeable in the binary model, with an increase of 0.0309.

## Rotation

While the binary and RMSprop models showed an increase in validation loss with rotation, the categorical models, both with and without class weights, demonstrated a decrease in validation loss when low (90 degrees) and high (180 degrees) rotations were applied, as shown in Figure 2 (Bottom Row). Models with random image rotation experienced the highest validation loss across all models.

```{r sumtable, eval=T, include=T, echo=F, align="center", fig.cap="Summary Table of Validation Loss for All CNN Models.", fig.pos="H"}
sumtable <- readImage("images/table_summary.png")
display(sumtable, method="raster")
```

## Combining Data Augmentations

```{r learningcurve, eval=F, include=T, echo=F, align="center", fig.cap="Learning Curves of Models with Combined Data Augmentation Compared to Models with No Noise", fig.pos="H"}
lcurve <- readImage("images/combined_learning_curve.png")
display(lcurve, method = "raster")
```

The combination of all data augmentations resulted in a reduction of validation loss across all models, except for the RMSprop model without class weights, as indicated in the summary table presented in Figure 3. 


# Results - Deployment 

All the plots above can be accessed through the [**CCR Shiny App**](https://usyd510436290.shinyapps.io/Image_App/). Thiss computer vision tool was development for the healthcare practitioners and academic researches in this field. It consists of six tabs: the introduction tab, three separate tabs for a brief description of each noise and interactive bar chart results, one tab for the interactive visualisation of learning curves for every model used for this project, and one tab that demonstrates an application of such tool. All interactive visualisation tools were created using plotly package in R (reference to plotly).

## Introduction

The introduction tab provides an overview of on the usage of the application, designed as a navigation guide for first time users.

## Choise of Noise

Within the noise description and results tabs, each augmentation was accompanied by examples showcasing its effect on a cell image, allowing users to compare it with the original image. This visual representation served to illustrate the impact of the augmentation for users who may be unfamiliar with the technique. Subsequently, a justification for the selection of each augmentation was provided, explaining the reasoning behind its inclusion. Additionally, an interactive bar chart was utilised to compare the validation loss of the models with the implemented augmentation. By hovering over each bar in the interactive plot, users can access the specific validation loss value. 

## Interactive Learning Curve Visualisation

This particular tab offers an interactive visualisation of the learning curves for all the models employed in this project. By default, the plot displays the validation loss over 100 epochs for models trained without any noise augmentation. However, users have the freedom to select and compare different models of their choice. This interactive feature enables users to gain insights into the impact of various augmentations, model optimisers, and loss functions on the learning process of CNN models. 

## Demonstration

The demonstration tab provides users with the opportunity to upload a cell image and experiment with various data augmentations, such as different levels of Gaussian noise, rotation, and resolution changes. By applying these augmentations simultaneously, users can observe the resulting predicted cluster in the output tab. This interactive feature enables users to gain a better understanding of how different data augmentations affect the model's predictions, showcasing one of the many practical applications of this tool. All predictions are generated using the Combined CNN model, utilising categorical cross-entropy as the loss function. 

# Discussion

An unexpected discovery arising from the results was that the inclusion of Gaussian noise resulted in an increase in validation loss across the models, suggesting overfitting tendencies. This finding contradicted previous research (reference to literature), which indicated that the implementation of Gaussian noise typically mitigates overfitting in CNN models. As depicted in Figure 4, one possible explanation for this outcome could be that the addition of Gaussian noise did not alter the predicted cluster of the cell image, unlike other augmentations that resulted in a discernible change in the predicted cluster. (Compare our implementation to other literatures, and conclude that future improvement can come from using the other implementation).

```{r demo_noise, echo=F, out.width="65%", out.height="30%",fig.cap="caption",fig.show='hold',fig.align='center', fig.cap="Demonstration tab on the CCR Shiny App. Image with no noise (Top). Image with 0.2 mean Gaussian noise (Middle). Image with 0.8 Gaussian noise (Bottom). The predicted cluster output remained the same."}
knitr::include_graphics(c("images/demo_nonoise.png","images/demo_02noise.png", "images/demo_08noise.png"))
```


A similar observation was made regarding the rotation augmentation, as it resulted in an increase in validation loss for the CNN models employing binary cross-entropy loss functions and RMSprop optimizers. This could be attributed to the enlarged size and increased variation of the training set, which slowed down the learning process for the models. Specifically, the models utilising binary cross-entropy loss functions may have been more susceptible to this effect, as this type of loss function does not penalise misclassifications as significantly as other loss functions (literature?). This issue was mitigated through employing a categorical cross-entropy loss function, which led to a decrease in validation loss was when this augmentation was applied, as can be seen in Figure 2 (Bottom row). The categorical cross-entropy loss function is better suited for multi-class classification problems and encourages the model to more accurately predict the correct cluster (literature).

The incorporation of low and medium resolutions resulted in a decrease in validation loss for all models. This can be attributed to the reduction in resolution, which enables the model to capture a more generalized shape of the cells, thereby improving the overall prediction generalization. (Supported by literature). However, when a random choice between low and medium resolutions was implemented, an increase in validation loss was observed for the binary and RMSprop models.This could be attributed to similar challenges encountered during the implementation of rotation augmentation for the aforementioned models.

As summarised in Figure 3, all models with class weights experiences a significant decrease in validation loss. This trend is particularly pronounced in the binary models, which experienced an average decrease of 0.05887 (23.10%) in validation loss. This finding demonstrated the effectiveness of class weights in addressing class imbalance within datasets, as it assigned higher weights to classes with lower counts. Furthermore, models utilising the RMSprop optimiser without class weights display an average decrease of 0.05842 (23.04%) in validation loss. Also, models employing the RMSprop optimizer in conjunction with class weights demonstrate an average decrease of 0.0239 (12.45%) in validation loss. (Literature on why RMSprop optimiser is better than Adam optimiser in this case).

By combining the optimal levels of each augmentation, substantial enhancements were achieved in the model's robustness. Specifically, the most significant improvements were observed in the models utilising categorical cross-entropy, with and without class weights, resulting in a decrease of 1.3457 (21.87%) and 0.8985 (16.29%) in validation loss, respectively, as depicted in Figure 3. This finding aligns with previous literature (reference to literature), which highlights that increasing the training size enhances data variability, providing the model with a wider range of examples to learn from. It also enabled the models to capture a more comprehensive representation of the underlying patterns (literature?). 

```{r conf_matrix, echo=F, out.width="65%", out.height="30%",fig.cap="caption",fig.show='hold',fig.align='center', fig.cap="Confusion Matrix on Validation Set for Original CNN model (Left) and Combined CNN model (Right). Both models used Categorical Cross-Entropy as the loss function."}
knitr::include_graphics(c("images/Matrix_1.png","images/Matrix_2.png"))
```

In addition, as observed in Figure 5, despite a higher error rate in the confusion matrix of the model incorporating combined augmentations and utilising the categorical cross-entropy loss function, it did not exhibit a bias towards clusters with a high class count. This is particularly evident in the model's predictions for cluster 1 and cluster 2, which had the highest class counts among all clusters. The model made more predictions, although inaccurate, for classes with lower class counts, demonstrating the effectiveness of implementing class weights. This finding indicates that the model learned to assign appropriate weights to different classes, thereby mitigating the impact of class imbalance and ensuring fair representation of all classes in the predictions. (This is supported by previous literature?). Employing a CNN model with a more complex architecture is likely to result in more accurate predictions for classes with lower counts.

The deployment of the CCR Shiny App not only provides valuable insights to researchers in the field but also serves as a practical tool for healthcare practitioners. By showcasing the application of cell image classification, the demonstration tab offers healthcare practitioners a glimpse into the potential of such a tool in their own practice. The Shiny App ultimately provides access to the study's results, enabling stakeholders and anyone interested to explore and draw their own inferences from the findings. This promotes transparency, collaboration, and the dissemination of knowledge in the field, aiding researchers and practitioners in making informed decisions and advancing their own studies.

## Limitation

Due to the simplicity of the CNN model used in this study and the high number of clusters in the dataset, the available metrics for effective interpretation were limited. Validation accuracy could not be relied upon since all models consistently showed low validation accuracy. The learning curves of the models also indicated overfitting tendencies, as evidenced by a continuous increase in validation loss over epochs. 

The generalisability of the results obtained in this experiment is constrained by the fact that the data was sourced from a single mouse brain. This limitation introduces a potential bias and restricts the ability to make broader generalizations about cell image classification across different samples or species.

The limited computational resources available for this study restricted the scope of augmentation testing and model experimentation. As a result, only a select set of augmentations and model variations could be explored within the given constraints.

# Conclusion

The inclusion of Gaussian noise typically resulted in an increase in validation loss, as did the introduction of rotation. However, the application of categorical cross-entropy loss function proved effective in mitigating these effects, leading to a reduction in validation loss. Similarly, the utilisation of lower and medium resolutions contributed to a decrease in validation loss across all models. Notably, the incorporation of class weights demonstrated its effectiveness in addressing class imbalance, resulting in decreased validation loss. The implementation of RMSprop optimiser also helped to minimize validation loss compared to the Adam optimiser.

Through combining the best performing noise levels into one training set, an overall decrease in validation loss was observed, demonstrating its ability to improve the robustness of the model to variations in input image quality.

## Future Work

To overcome the limitations of this study, several areas can be further developed. Firstly, increasing computational resources would enable the investigation of additional augmentations, providing a more comprehensive understanding of their effects. Moreover, with enhanced resources, a wider range of model variations can be explored, facilitating more in-depth analyses. The utilisation of deeper CNN architectures would allow for the incorporation of additional evaluation metrics, leading to more comprehensive insights. To enhance the reliability of results, it is recommended to run the models multiple times and record the average outcomes, ensuring more consistent and robust findings.

Expanding the dataset by gathering images from multiple mouse brains would significantly enhance the robustness of the results. This approach would not only help address the issue of class imbalance within the dataset but also provide a broader and more generalised interpretation of the findings. By incorporating data from diverse sources, potential biases specific to a single mouse brain can be mitigated, leading to more reliable and representative outcomes.

\newpage

\footnotesize


# Student Contributions 




# References





\newpage

# Appendix 

Main code & technical details of your approach
