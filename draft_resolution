library(keras)
library(tensorflow)

# Define image size and batch size
img_size <- c(64, 64)
batch_size <- 32

# Load data
train_data <- flow_images_from_directory(
  "cell_images/train",
  target_size = img_size,
  batch_size = batch_size,
  class_mode = "categorical"
)
test_data <- flow_images_from_directory(
  "cell_images/test",
  target_size = img_size,
  batch_size = batch_size,
  class_mode = "categorical"
)

# Rescale pixel values to between 0 and 1
train_data <- train_data / 255
test_data <- test_data / 255


library(magick)

resize_image <- function(image, resolution) {
  new_size <- resolution * img_size
  image_resize(image, paste(new_size[1], new_size[2], sep = "x"), filter = "bicubic")
}

# Define CNN model
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", input_shape = c(64, 64, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 2, activation = "softmax")

# Compile the model
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(),
  metrics = c("accuracy")
)

# Train the model
history <- model %>% fit(
  train_data,
  epochs = 10,
  validation_data = test_data
)

# Evaluate the model on test data
test_metrics <- model %>% evaluate(test_data)


# Use the resolution function to resize the test data to different resolutions
test_resized <- lapply(resolutions, function(res) {
  resize_images(test_images, res)
})

# Evaluate the performance of the trained model on each resized version of the test data
test_metrics <- lapply(test_resized, function(test_data) {
  evaluate(model, test_data, test_labels)
})
